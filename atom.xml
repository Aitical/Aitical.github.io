<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aitical的个人小站</title>
  
  <subtitle>一直在走,走灰多少太阳</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-16T11:20:00.316Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>吴刚</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://yoursite.com/2018/04/16/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://yoursite.com/2018/04/16/朴素贝叶斯/</id>
    <published>2018-04-16T08:28:13.000Z</published>
    <updated>2018-04-16T11:20:00.316Z</updated>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯的理论基础"><a href="#朴素贝叶斯的理论基础" class="headerlink" title="朴素贝叶斯的理论基础"></a>朴素贝叶斯的理论基础</h2><h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p> 首先看一下概统中的条件概率,在B条件下事件A发生的概率<br>$$<br>P(A|B)=\frac{P(AB)}{P(B)}<br>$$<br>贝叶斯定理便是基于条件概率,通过$P(A|B)$求得$P(B|A)$<br>$$<br>P(B|A) =\frac{P(A|B)P(B)}{P(A)}<br>$$<br>在给定数据集$(X,Y)$中,每个样本$x$有$n$个特征$(x_{1}, x_{2}, x_{3},\cdots,x_{n})$,</p><p>标记中有$k$个类别$y=(y_{1}, y_{2},\cdots,y_{n})$.</p><p>这时给定新样本$x$,如何判断其所属的类别便是给定$x$条件下$max(P(y_{1}|x),P(y_{2}|x),cdots,P(y_{k}|x))$,</p><p>我们取概率最大的为结果标记.这时$P(y_{k}|x)$就是用贝叶斯公式:<br>$$<br>P(y_{k}|x)=\frac{P(x|y_{k})P(y_{k})}{P(x)}<br>$$<br>对公式进行一些变形和运算:</p><p>分子中的$P(y_{k})$根据数据集直接求出</p><p>$P(x|y_{k})=P(x_{1},x_{2},\cdots,x_{n}|y_{k})$,假设第$i$维的特征$x_{i}$可取的值有$S_{i}$个,类别取值个数为$k$个,那么参数个数也就是$K\prod_{i=1}^{n}S_{i}$个,这是指数级的个数,显然不行.</p><p>在这里,朴素贝叶斯算法对条件概率分布做出了独立性假设,简单的说也就是假设各个维度的特征$x_{1}, x_{2},\cdots,x_{n}$是相互独立的,此时条件概率转化为<br>$$<br>P(x|y_{k})=P(x_{1},x_{2},\cdots,x_{n}|y_{k})=\prod_{i=1}^{n}P(x_{i}|y_{k})<br>$$<br>分母部分</p><p>首先运用全概率公式:<br>$$<br>P(A) = \sum_{i=1}^{n}P(B_{i})P(A|B_{i})<br>$$<br>得到<br>$$<br>P(x)=\sum_{k}\space P(y_{k})P(x|y_{k})<br>$$<br>再运用上面的独立性假设<br>$$<br>P(x) = \sum_{k}\space P(y_{k})P(x|y_{k})=\sum_{k}P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})<br>$$<br>则<br>$$<br>P(y_{k}|x)=\frac{P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}{\sum_{k}P(y_{k}\prod_{i=1}^{n}P(x_{i}|y_{k})}<br>$$<br>于是朴素贝叶斯分类器可表示为<br>$$<br>f(x)=argmax_{y_{k}}P(y_{k}|x)=argmax_{y_{k}}\frac{P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})}{\sum_{k}P(y_{k}\prod_{i=1}^{n}P(x_{i}|y_{k})}<br>$$<br>又对于所有的$y_{k}$,式中的分母都一样,则忽略分母部分,最终可表示为:<br>$$<br>f(x)=argmax_{y_{k}}P(y_{k}|x)=argmax_{y_{k}}P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})<br>$$<br>下面将介绍几个基本模型用来计算朴素贝叶斯</p><h2 id="几种常见模型的实现"><a href="#几种常见模型的实现" class="headerlink" title="几种常见模型的实现"></a>几种常见模型的实现</h2><h3 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h3><p>当特征是离散值时,使用多项式模型.</p><p>多项式模型在计算先验概率$P(y_{k})$和条件概率$P(x_{i}|y_{k})$时会做一些平滑处理,具体为:<br>$$<br>P(y_{k})=\frac{N_{y_{k}}+\alpha}{N+k\alpha}<br>$$</p><p>$$<br>P(x_{i}|y_{k})=\frac{N_{y_{k}x_{i}}+\alpha}{N_{y_{k}}+n\alpha}<br>$$</p><p>其中,</p><p>$N$是数据集的总样本数</p><p>$N_{y_{k}}$是标签为$y_{k}$的样本数,$N_{y_{k}x_{i}}$是标签为$y_{k}$中特征为$X_{i}$的样本数</p><p>$k$的标签总数, $n$是特征维度</p><p>$\alpha$是平滑系数</p><p>当$\alpha=1$时称为$Laplace$平滑,当$0&lt;\alpha&lt;1$时称$Lidstone$平滑,$\alpha=0$时不平滑</p><p>平滑处理可以避免遇到训练数据中没出现过的$x_{i}$时,$P(x_{i}|y_{k})=0$从而导致后验概率也为0.</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>当特征值是连续变量时,多项式模型会当做离散值处理从而导致很多$P(x_{i}|y_{k})=0$出现(未平滑),及时进行了平滑处理,效果也不理想,这时可以采用高斯模型</p><p>高斯模型假设其中的每一维特征值都符合正太分布</p><p>比如体测信息中的身高数据,不能当做离散值计算时,我们可以宏观的看身高符合正太分布,则可以计算出身高数据的期望和方差,从而就知道了某一身高值的概率</p><p>高斯分布的概率密度函数:<br>$$<br>f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}<br>$$<br>高斯模型的计算:<br>$$<br>P(x_{i}|y_{k})=\frac{1}{\sigma_{y_{k},i}\sqrt{2\pi}}e^{-\frac{(x-\mu_{y_{k},i})^{2}}{2 \sigma_{y_{k},i}^{2}}}<br>$$<br>其中</p><p>$\mu_{y_{k},i}$表示标签$y_{k}$中的第$i$维特征值的期望</p><p>$\sigma_{y_{k},i}$是标签为$y_{k}$中的第$i$维特征值的标准差</p><p>也就是运用正态分布的概率密度函数,对每一组条件概率下都进行计算.</p><p>代码和对应的例子之后补更.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;朴素贝叶斯的理论基础&quot;&gt;&lt;a href=&quot;#朴素贝叶斯的理论基础&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯的理论基础&quot;&gt;&lt;/a&gt;朴素贝叶斯的理论基础&lt;/h2&gt;&lt;h3 id=&quot;贝叶斯定理&quot;&gt;&lt;a href=&quot;#贝叶斯定理&quot; class=&quot;he
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯" scheme="http://yoursite.com/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="统计学习方法" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>概率论基础</title>
    <link href="http://yoursite.com/2018/04/14/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/04/14/概率论基础/</id>
    <published>2018-04-14T06:40:10.000Z</published>
    <updated>2018-04-14T07:57:19.899Z</updated>
    
    <content type="html"><![CDATA[<h2 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h2><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>设$P(x)$是一个离散概率分布函数, 自变量的取值范围为$[x_{1}, x_{2}, x_{3},\dots,x_{n}]$其期望定义为:<br>$$<br>E(x)=\sum_{k=1}^{n}x_{k}P(x_{k})<br>$$<br>若$P(x)$是一个连续型概率密度函数,其期望为:<br>$$<br>E(x)=\int_{-\infty}^{+\infty}xP(x)dx<br>$$</p><h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><p><strong>线性运算规则</strong>: 线性运算的期望等于期望的线性运算<br>$$<br>E(ax+by+c) = aE(x)+bE(y)+c<br>$$<br>更一般的表达形式:<br>$$<br>E(\sum_{k=1}^{n}a_{k}x_{k}+c) = \sum_{k=1}^{n}a_{k}E(x_{k}) +c<br>$$<br><strong>乘积的期望</strong></p><p>乘积的期望不等于期望的乘积,当且仅当变量相互独立时,如变量x,y相互独立,才有$E(xy)=E(x)E(y)$</p><h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>方差是一种特殊的期望:<br>$$<br>Var(x) = E((x-E(x))^{2})<br>$$</p><h4 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h4><p><strong>另一种表示形式</strong></p><p>根据期望的性质对方差公式进行展开运算:<br>$$<br>\begin{align}<br>Var(x) &amp;= E(x^{2}-2xE(x)+(E(x))^{2}) \\<br>&amp;=E(x^{2})-2E(x)E(x)+(E(x))^{2} \\<br>&amp;=E(x^{2})-(E(x))^{2}<br>\end{align}<br>$$<br><strong>常数方差</strong></p><p>设$C$是常数,则$Var(C)=0$</p><p><strong>一些运算性质</strong></p><p>设随机变量$X$方差存在$Var(X),\space a,b$为常数则:<br>$$<br>Var(aX+b) = a^{2}Var(X)<br>$$<br>设随机变量$X,Y$方差存在,则<br>$$<br>Var(aX+bY) = a^{2}Var(X)+b^{2}Var(Y)+2Cov(X,Y)<br>$$<br>其中$Cov(X,Y)$是协方差</p><p>当变量间相互独立时,可以推广得到<br>$$<br>Var(\sum_{k=1}^{n}a_{k}X_{k}) = \sum_{k=1}^{n}a_{k}^{2}Var(X_{k})<br>$$</p><h2 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h2><h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><p>$$<br>Cov(x,y) = E((x-E(x))(y-E(y)))<br>$$</p><h4 id="性质-2"><a href="#性质-2" class="headerlink" title="性质"></a>性质</h4><p><strong>另一种形式</strong><br>$$<br>Cov(x,y)=E(xy)-E(x)E(y)<br>$$<br><strong>方差是一种特殊的协方差</strong><br>$$<br>Cov(x,x) = Var(x)<br>$$<br><strong>线性组合的协方差</strong><br>$$<br>Cov(\sum_{k=1}^{n}a_{k}x_{k}, \sum_{j=1}^{m}b_{j}y_{j}) = \sum_{k=1}^{n}\sum_{j=1}^{m}a_{k}b_{j}Cov(x_{k}, y_{j})<br>$$<br>一种特殊情况:<br>$$<br>Cov(a+bx, c+dy) = dbCov(x,y)<br>$$</p><h2 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h2><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><p>设随机变量X,Y期望和方差都存在,则X与Y的相关系数:<br>$$<br>\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{Var(x)}\sqrt{Var(Y)}}<br>$$</p><h4 id="性质-3"><a href="#性质-3" class="headerlink" title="性质"></a>性质</h4><p><strong>有界性</strong></p><p>$|\rho_{XY}|\le1$</p><p><strong>统计意义</strong></p><p>越趋近1则说明正相关越强</p><p>越趋近-1则是负相关性越强</p><p>0是非线性相关</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;期望&quot;&gt;&lt;a href=&quot;#期望&quot; class=&quot;headerlink&quot; title=&quot;期望&quot;&gt;&lt;/a&gt;期望&lt;/h2&gt;&lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;设$P
      
    
    </summary>
    
      <category term="概率统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="概率统计" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="相关系数" scheme="http://yoursite.com/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2018/03/25/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2018/03/25/决策树/</id>
    <published>2018-03-25T15:32:44.000Z</published>
    <updated>2018-03-29T13:25:31.228Z</updated>
    
    <content type="html"><![CDATA[<h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p>分类决策树模型是一种描述对实例进行分类的属性结构.决策树由结点(node)和有向边(directed edge)组成.结点有两种类型,内部结点(internal node)和叶结点(leaf node).内部结点表示一个特征或属性,也结点表示一个类.<br>用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,将实例分配到其子结点:这是,每一个子结点对应着该特征值的一个取值.如此递归的对实例进行测试并分配,直到达到叶结点.最后将实例分到叶结点的类中.</p><h2 id="特征选择-数学基础"><a href="#特征选择-数学基础" class="headerlink" title="特征选择(数学基础)"></a>特征选择(数学基础)</h2><p>特征选择在于选取对训练数据具有分类能力的特征.通常特征选择的准则是信息增益(ID3算法)或信息增益比(C4.5算法).</p><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>熵(entropy)表示随机变量不确定性的度量.<br>设$X$是一个取有限个值的离散变量,其概率分布为:<br>$$<br>P(X=x_{i})=p_{i}, \space i=1,2,3, \cdots, n<br>$$<br>则随机变量$X$的熵的定义为:<br>$$<br>H(X)=-\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$<br>若$p_{i}=0$则定义$0\space log\space0=0$<br><br>式中的对数以2为底或以$e$为底,这时结果的单位分别称作比特(bit)或者纳特(nat)<br>由定义知,熵只依赖于$X$的分布,而与$X$的取值无关,所以也可以将$X$的熵记作$H(p)$,即:<br>$$<br>H(p) = -\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$<br>熵越大,随机变量的不确定性越大.从定义可验证:<br>$$<br>0\leq H(p)\leq log\space n<br>$$<br>当随机变量只取两个值, 例如$1,0$时,即$X$的分布为:<br>$$<br>P(X=1) = p, \space \space P(X=0) = 1-p, \space 0\leq 1<br>$$<br>熵为:<br>$$<br>H(p) = -p\space log_2\space p-(1-p)log_{2}\space (1-p)<br>$$<br>这时熵$H(p)$随概率$p$的变化曲线如下图所示(单位为比特):</p><p><img src="/2018/03/25/决策树/H.png" alt="H(p)"></p><p>当$p=0$或$p=1$时$H(p)=0$,随机变量完全没有不确定性.当$p=0.5$时,$H(p)=1$,熵取值最大,随机变量不确定性最大.<br>简单的来说,熵就是事件发生概率的<strong>负对数的期望</strong><br>$$<br>E(X) = -\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$</p><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>设有随机变量$(X,Y)$, 其联合概率分布为:<br>$$<br>P(X=x_{i}, Y=y_{j})=p_{ij}, \space i=1,2,\cdots\,n;\space j=1,2,\cdots,m<br>$$<br>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性,随机变量$X$给定的条件下$Y$的条件熵(conditional entropy)$H(Y|X)$,定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望<br>$$<br>H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})<br>$$<br>这里,$p_{i}=P(X=x_{i}), i=1,2,\cdots,n$<br>当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵与条件熵分别称为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy).此时,如果有0概率,令$0\space log\space0=0$</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>信息增益(information gain)表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度.<br>特征$A$对训练数据集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下经验条件熵$H(D|A)$之差,即<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br>一般地,熵$H(Y)$与条件熵$H(Y|X)$的差称为互信息(mutual information).决策树学习中的信息增益等价于训练数据集中类与特征的互信息.</p><p>决策树学习应用信息增益准则选择特征.给定训练数据集$D$和特征$A$,经验熵$H(D)$表示对数据集$D$进行分类的不确定性,而经验条件熵$H(D|A)$表示特征$A$给定条件下$D$的不确定性,那么他们的差,即信息增益就是表示由于特征$A$而使$D$的分类不确定性减少的程度.<br>根据信息增益准则的特征选择方法是:</p><p>对训练数据集$D$,计算其每个特征的信息增益,并比较他们的大小,选择信息增益最大的特征.</p><p>设训练数据集为$D$,$|D|$表示其样本容量,即样本的个数.</p><p>设有$K$个类$C_{k}, k=1,2,\cdots,K$,$|C_{k}|$是属于类$|C_{k}|$的样本个数, $\sum_{k=1}^{K}|C_{k}|=|D|$.</p><p>设特征$A$有$n$个不同取值$\lbrace a_{1},a_{2},\cdots,a_{n}\rbrace $,根据特征$A$的取值将$D$划分为$n$个子集$D_{1}, D_{2}, \cdots, D_{n}$,$|D_{i}|$为$D_{i}$样本个数,$\sum_{i=1}^{n}|D_{i}|=|D|$.</p><p>记子集$D_{i}$中属于类$C_{k}$的样本集合为$D_{ik}$,即$D_{ik}=D_{i}\cap C_{k}$</p><p>信息增益算法:</p><ol><li>计算数据集$D$的经验熵$DH(D)$<br>$$<br>H(D) = -\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\space\frac{|C_{k}|}{|D|}<br>$$</li><li>计算特征$A$对数据集$D$的经验条件熵<br>$$<br>H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{i=1}^{n}\frac{|D_{ik}|}{|D_{i}|}log_{2}\space\frac{|D_{ik}|}{|D_{i}|}<br>$$</li><li>计算信息增益<br>$$<br>g(D,A) = H(D)-H(D|A)<br>$$</li></ol><p>举个例子:</p><table><thead><tr><th>ID</th><th>age</th><th>hasWork</th><th>hasOwnHouse</th><th>credit</th><th>clazz</th></tr></thead><tbody><tr><td>1</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>2</td><td>青年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>3</td><td>青年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>4</td><td>青年</td><td>是</td><td>是</td><td>一般</td><td>是</td></tr><tr><td>5</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>6</td><td>中年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>7</td><td>中年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>8</td><td>中年</td><td>是</td><td>是</td><td>好</td><td>是</td></tr><tr><td>9</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>10</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>11</td><td>老年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>12</td><td>老年</td><td>否</td><td>是</td><td>好</td><td>是</td></tr><tr><td>13</td><td>老年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>14</td><td>老年</td><td>是</td><td>否</td><td>非常好</td><td>是</td></tr><tr><td>15</td><td>老年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr></tbody></table><p>根据信息增益准则选择最优特征:<br>首先看到数据最终有两类,也就是公式中$k=2$<br>先计算经验熵$H(D)$<br>$$<br>H(D) = -\frac{9}{15}log_{2}\frac{9}{15}-\frac{6}{15}log_{2}\frac{6}{15}=0.971<br>$$<br>然后计算$D$对各个特征的经验条件熵,四个特征分别记作$A_{1}, A_{2}, A_{3}, A_{4}$</p><p>可以看到$A_{1}$特征有3个,恰好是均分的,每个对应5条记录</p><p>$$<br>\begin{align}<br>g(D, A_{1}) &amp;= H(D)-\left[ \frac{5}{15}H(D_{1}) +\frac{5}{15}H(D_{2}) +\frac{5}{15}H(D_{3})\right] \\ &amp;=0.971-\left[ \frac{5}{15}\left( -\frac{2}{15}log_{2}\frac{2}{15}-\frac{3}{15}log_{2}\frac{3}{15}\right)+ \\<br>\frac{5}{15}\left( -\frac{3}{15}log_{2}\frac{3}{15}-\frac{2}{15}log_{2}\frac{2}{15}\right)+ \\<br>\frac{5}{15}\left( -\frac{4}{15}log_{2}\frac{4}{15}-\frac{1}{15}log_{2}\frac{1}{15}\right)\right] \\<br>&amp;=0.971-0.888=0.083<br>\end{align}<br>$$<br>同样的求得<br>$$<br>g(D, A_{2}) = 0.324, \space g(D, A_{3}) = 0.420,\space g(D,A_{4}) =0.363<br>$$<br>比较就可以看出$A_{3}$的信息增益最大,所以选择$A_{3}$作为最优特征</p><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>以信息增益作为划分训练数据集的特征,存在偏向于选择取值较多的特征的问题.使用信息增益比(information gain ratio)可以对这一问题进行校正.</p><p>这个地方要多体会一下,具体的看,上述例子中,如果我把ID字段也作为一个特征$A_{0}$进行处理的话,毫无疑问,最优特征一定是$A_{0}$,<br>因为ID字段每一条记录都唯一,也就是说此时的条件经验熵:<br>$$<br>H(D|A_{0}) = -\sum_{i=1}^{15}log_{2}(1)=0<br>$$<br>信息增益取得最大值$H(D)$<br>这也就是说信息增益总是会倾向于取值较多的特征项,尤其是连续性特征,每一个数值都会记作唯一的,这个时候信息增益比就可以进行校正.</p><p>特征$A$对训练数据集$D$的<strong>信息增益比</strong>$g_{R}(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_{A}(D)$之比.即<br>$$<br>g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}<br>$$<br>其中<br>$$<br>H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}<br>$$<br>$n$是特征A的取值个数,$D_{i}$是特征$A$取第$i$个时的样本个数</p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><p>ID3判断的依据就是信息增益<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure></p><p>计算经验熵<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calEnt</span><span class="params">(dataframe)</span>:</span></span><br><span class="line">    classes = dataframe.groupby(by=data.columns[<span class="number">-1</span>]).size().values</span><br><span class="line">    total = classes.sum()</span><br><span class="line">    classes_pos = classes/total</span><br><span class="line">    ent = -(classes_pos * np.log2(classes_pos)).sum()</span><br><span class="line">    <span class="keyword">return</span> ent</span><br></pre></td></tr></table></figure></p><p>选择最优特征<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeat</span><span class="params">(dataframe)</span>:</span></span><br><span class="line">    total = len(dataframe)</span><br><span class="line">    Hd = calEnt(dataframe)</span><br><span class="line">    final_res = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> dataframe.columns[:<span class="number">-1</span>]:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> dataframe[feature].unique():</span><br><span class="line">            df = dataframe[dataframe[feature]==key]</span><br><span class="line">            result += len(df)/total * calEnt(df)</span><br><span class="line">        final_res.append([feature, total - result])</span><br><span class="line">    final_res.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> final_res[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>判断分类结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseLabel</span><span class="params">(labels)</span>:</span></span><br><span class="line">    print(labels)</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(labels).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>创建树,使用字典实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(df.columns)==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> chooseLabel(list(df.iloc[:,<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">if</span> len(df.iloc[:,<span class="number">-1</span>].unique()) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> df.iloc[<span class="number">0</span>,<span class="number">-1</span>]</span><br><span class="line">    bestFeature = chooseBestFeat(df)</span><br><span class="line">    Tree = &#123;bestFeature: &#123;&#125;&#125;</span><br><span class="line">    features = [i <span class="keyword">for</span> i <span class="keyword">in</span> df.columns <span class="keyword">if</span> i != bestFeature]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> df[bestFeature].unique():</span><br><span class="line">        df_ = df[df[bestFeature]==key][features]</span><br><span class="line">        Tree[bestFeature][key]=createTree(df_)</span><br><span class="line">    <span class="keyword">return</span> Tree</span><br></pre></td></tr></table></figure></p><p>分类器的实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(Tree, df)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> isinstance(Tree, dict):</span><br><span class="line">        feature = list(Tree.keys())[<span class="number">0</span>]</span><br><span class="line">        key = df[feature].values[<span class="number">0</span>]</span><br><span class="line">        Tree = Tree[feature][key]</span><br><span class="line">    <span class="keyword">return</span> Tree</span><br></pre></td></tr></table></figure></p><p>储存/读取建立好的树<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(Tree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(Tree, f)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grapTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(f)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;决策树模型&quot;&gt;&lt;a href=&quot;#决策树模型&quot; class=&quot;headerlink&quot; title=&quot;决策树模型&quot;&gt;&lt;/a&gt;决策树模型&lt;/h2&gt;&lt;p&gt;分类决策树模型是一种描述对实例进行分类的属性结构.决策树由结点(node)和有向边(directed edge)组成
      
    
    </summary>
    
      <category term="统计学习方法" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>KNN与Kd-tree实现</title>
    <link href="http://yoursite.com/2018/03/21/KNN%E4%B8%8EKd-tree%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/03/21/KNN与Kd-tree实现/</id>
    <published>2018-03-21T14:09:41.000Z</published>
    <updated>2018-03-28T12:38:23.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-近邻算法学习"><a href="#K-近邻算法学习" class="headerlink" title="K-近邻算法学习"></a>K-近邻算法学习</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述:"></a>算法描述:</h2><p>KNN原理也就是常说的物以类聚,人以群分.当无法判断当前点所属分类时,可以通过统计它所属的位置特征,衡量它周围的邻居权重,把分配到权重大的一类.</p><p>首先要知道变量之间的距离的表示:<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{p}\right)^{\frac{1}{p}}<br>$$<br>其中p=2时就是欧氏距离<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{2}\right)^{\frac{1}{2}}<br>$$</p><p>p=1时称曼哈顿距离:<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|\right)<br>$$</p><p>p=$\propto$时, 它是各个坐标距离的最大值:<br>$$<br>L_{\propto}(x_{i}, x_{j})=max(|x_{i}^{(l)}-x_{j}^{l}|)<br>$$</p><p>KNN算法中,所选择对象的邻居对象都是已经正确分类的.该方法在定类的决策中只依据最近的一个邻居或者几个来决定所属类别.所以这里面K的取值还是很关键的.太小则容易受到训练数据的噪声影响从而产生过拟合,太大则有可能会误会测试,因为这样会使近邻中包含了远离的点.</p><p>算法描述:</p><ol><li>K(近邻数), D(训练样本集合)</li><li>z(x, y)待测样本</li><li>遍历所有样本数据,计算z到样例之间的距离d</li><li>选择最近的k个样例集合 </li></ol><h2 id="knn算法的实现"><a href="#knn算法的实现" class="headerlink" title="knn算法的实现"></a>knn算法的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(X_train, y_train, X_test, k)</span>:</span></span><br><span class="line">    m = len(X_test)</span><br><span class="line">    y_pre = np.empty((m,),dtype=object)</span><br><span class="line">    <span class="keyword">for</span> i, test <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">        neighbour = []</span><br><span class="line">        <span class="keyword">for</span> j, train <span class="keyword">in</span> enumerate(X_train):</span><br><span class="line">            dist = np.linalg.norm(test - train)</span><br><span class="line">            label = y_train[j]</span><br><span class="line">            neighbour.append([dist, label])</span><br><span class="line">        neighbour = np.array(neighbour)</span><br><span class="line">        k_neighbour = neighbour[neighbour[:,<span class="number">0</span>].argsort()][:k][:,<span class="number">1</span>]</span><br><span class="line">        y_pre[i] = pd.value_counts(k_neighbour).index[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> y_pre</span><br></pre></td></tr></table></figure><h2 id="简单的算法使用"><a href="#简单的算法使用" class="headerlink" title="简单的算法使用:"></a>简单的算法使用:</h2><h3 id="1-鸢尾花类别判断"><a href="#1-鸢尾花类别判断" class="headerlink" title="1. 鸢尾花类别判断"></a>1. 鸢尾花类别判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'iris.data'</span>, header=<span class="keyword">None</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]].values</span><br><span class="line">y = df[<span class="number">4</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = sns.pairplot(df, hue= <span class="number">4</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="keyword">True</span>),plot_kws=dict(s=<span class="number">10</span>),)</span><br></pre></td></tr></table></figure><p><img src="/2018/03/21/KNN与Kd-tree实现/iris.png" alt="iris"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_pre, y_test)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(y_pre == y_test)/ len(y_test)</span><br><span class="line"></span><br><span class="line">y_pre = knn(X_train, y_train, X_test, <span class="number">5</span>)</span><br><span class="line">accuracy_score(y_pre, y_test)</span><br></pre></td></tr></table></figure><h3 id="2-改进约会网站的匹配"><a href="#2-改进约会网站的匹配" class="headerlink" title="2. 改进约会网站的匹配"></a>2. 改进约会网站的匹配</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'datingTestSet.txt'</span>, sep=<span class="string">'\t'</span>, header=<span class="keyword">None</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>可视化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">g = sns.pairplot(df, hue=<span class="number">3</span>, size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="keyword">True</span>),plot_kws=dict(s=<span class="number">10</span>),)   X = df[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]].values</span><br><span class="line">y = df[<span class="number">3</span>].values</span><br></pre></td></tr></table></figure></p><p><img src="/2018/03/21/KNN与Kd-tree实现/dating.png" alt="约会网站人群"></p><p>标准化数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Normalization</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (X-X.mean(axis=<span class="number">0</span>))/X.std(axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">X = Normalization(X)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line">y_pre = knn(X_train, y_train, X_test, <span class="number">5</span>)</span><br><span class="line">accuracy_score(y_pre, y_test)</span><br></pre></td></tr></table></figure></p><h3 id="3-手写字识别"><a href="#3-手写字识别" class="headerlink" title="3. 手写字识别"></a>3. 手写字识别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">train_files = os.listdir(<span class="string">'digits/trainingDigits'</span>)</span><br><span class="line">test_files = os.listdir(<span class="string">'digits/testDigits'</span>)</span><br></pre></td></tr></table></figure><p>把图像内容转换成数组形式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgVector</span><span class="params">(files)</span>:</span></span><br><span class="line">    m = len(files)</span><br><span class="line">    X = np.empty((m, <span class="number">1024</span>), dtype=int)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    y = np.empty((m,), dtype=object)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        y[count] = file[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'digits/trainingDigits/'</span>+file) <span class="keyword">as</span> fin:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">                line = fin.readline()</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">                    X[count, <span class="number">32</span>*i+j] = int(line[j])</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre></td></tr></table></figure></p><p>读取文件并执行knn<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train, y_train = imgVector(train_files) </span><br><span class="line">X_test, y_test = imgVector(test_files)</span><br><span class="line"></span><br><span class="line">y_pre = knn(X_train, y_train, X_test, <span class="number">5</span>)</span><br><span class="line">accuracy_score(y_pre, y_test)</span><br></pre></td></tr></table></figure></p><h2 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h2><p>上面的例子中采用的是循环遍历的方法实现了距离的计算,在数据量很大的时候这样的遍历显然会带来性能的瓶颈,当然也可以对数据进行向量化计算来提升速度,不过这里我们引入kd树这一种数据结构来存储数据并进行knn</p><p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树是二叉树,表示k维空间的一个划分(partition).构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分,构成一系列的k维超矩形区域,kd树的每个节点对应于一个k维超矩形区域.</p><p><strong>算法: 构造平衡kd树<br></strong><br>    输入: k维空间数据集$T=\lbrace{ x_{1}, x_{2}, \dots, x_{N} \rbrace}$<br> 其中$x_{i} = \left( x_{i}^{(1)}, x_{i}^{(2)}, \dots, x_{i}^{(k)}\right), i=1, 2, \dots, N$<br><br>    输出: kd树</p><p>开始: 构造根节点, 选择$x^{(1)}$为坐标轴,以T中所有实例的$x^{(1)}$坐标的中位数为切分点,将根节点对应的超矩形区域切分成两个子区域.切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现</p><p>由根节点生成深度为1的左,右子结点: 左子结点对应的坐标$x^{(1)}$小于切分点的子区域,右子结点对应的坐标大于切分点的子区域</p><p>将落在切分面上的实例点保存在根节点</p><p>重复: 对于深度为$j$的结点, 选择$x^{(l)}$为切分的坐标轴<br>其中$l=(jmodK)+1$<br>以该结点的区域中所有实例的$x^{(i)}$坐标的中位数为切分点,将该结点对应的超矩形区域切分成两个子区域,切分由通过切分点并垂直于坐标轴$x^{(l)}$的超平面实现<br><br>由根节点生成深度为j+1的左,右子结点: 左子结点对应的坐标$x^{(l)}$小于切分点的子区域,右子结点对应的坐标大于切分点的子区域<br><br>将落在切分面上的实例点保存在根节点</p><p>直到两个子区域没有实例在时停止.</p><p>例:二维空间数据集:<br>$$<br>T=\lbrace{(2, 3)^{T},(5, 4)^{T},(9, 6)^{T},(4, 7)^{T},(8, 1)^{T},(7, 2)^{T}\rbrace}<br>$$</p><p>选择$x^{(1)}$轴,中位数是7,平面$x^{(1)}=7$把空间分成左右两个子矩形</p><p>左矩形中与$x^{(2)}=4$分成两个矩形,右矩形中$x^{(2)}=6$划分</p><p>重复上述操作就得到下面的示意图</p><p><img src="/2018/03/21/KNN与Kd-tree实现/kd-tree1.png" alt="划分"><br>树形结构如下:<br><br><img src="/2018/03/21/KNN与Kd-tree实现/kd-tree2.png" alt="kd-tree"></p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    结点类</span></span><br><span class="line"><span class="string">    ------------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    point: 当前节点的序列</span></span><br><span class="line"><span class="string">    split: 当前结点划分的维度</span></span><br><span class="line"><span class="string">    LL: 当前节点的左孩子</span></span><br><span class="line"><span class="string">    RR: 当前节点的右孩子</span></span><br><span class="line"><span class="string">    ------------</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, point=None, split=None, LL=None, RR=None)</span>:</span></span><br><span class="line">        self.point = point</span><br><span class="line">        self.split = split</span><br><span class="line">        self.left = LL</span><br><span class="line">        self.right = RR</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKdTree</span><span class="params">(root, arraydata)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    生成KD树</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    root: 结点指针</span></span><br><span class="line"><span class="string">    arraydata: 结点所属的划分区域</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    root: 结点</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    length = arraydata.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    dimension = arraydata.shape[<span class="number">1</span>]</span><br><span class="line">    var = np.std(arraydata, axis=<span class="number">0</span>)</span><br><span class="line">    split = np.where(var==var.max())[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    sortdata = arraydata[np.argsort(arraydata[:,split])]</span><br><span class="line">    median = sortdata.shape[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">    point = sortdata[median]</span><br><span class="line">    root = Node(point, split)</span><br><span class="line">    root.left = createKdTree(root.left, sortdata[:median,:] )</span><br><span class="line">    root.right = createKdTree(root.right, sortdata[median+<span class="number">1</span>:,:])</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KNN</span><span class="params">(root, aimlst, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    KNN</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    root: kd-tree根节点</span></span><br><span class="line"><span class="string">    aimlst: 待分类的目标序列</span></span><br><span class="line"><span class="string">    k: 选取的近邻数</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    node_K: 选中的k个近邻列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    node_dist = []</span><br><span class="line">    node_k = []</span><br><span class="line">    nodelst = []</span><br><span class="line">    temp_root = root</span><br><span class="line">    <span class="keyword">while</span> temp_root:</span><br><span class="line">        nodelst.append(temp_root)</span><br><span class="line">        distance = np.linalg.norm(aimlst - temp_root.point)</span><br><span class="line">        <span class="keyword">if</span> len(node_k) &lt; k:</span><br><span class="line">            node_dist.append(distance)</span><br><span class="line">            node_k.append(temp_root.point)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            max_dis = np.max(node_dist)</span><br><span class="line">            <span class="keyword">if</span> distance &lt; max_dis:</span><br><span class="line">                max_index = node_dist.index(max_dis)</span><br><span class="line">                node_dist[max_index] = distance</span><br><span class="line">                node_k[max_index] = temp_root.point</span><br><span class="line">        ss = temp_root.split</span><br><span class="line">        <span class="keyword">if</span> aimlst[ss] &lt;= temp_root.point[ss]:</span><br><span class="line">            temp_root = temp_root.left</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp_root = temp_root.right</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> nodelst:</span><br><span class="line">        back_point = nodelst.pop()</span><br><span class="line">        ss = back_point.split</span><br><span class="line">        max_dis = max(node_dist)</span><br><span class="line">        <span class="keyword">if</span> len(node_k) &lt; k <span class="keyword">or</span> abs(aimlst[ss] - back_point.point[ss])&lt; max_dis:</span><br><span class="line">            <span class="keyword">if</span> aimlst[ss] &lt;= back_point.point[ss]:</span><br><span class="line">                temp_root = back_point.right</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp_root = back_point.left</span><br><span class="line">            <span class="keyword">if</span> temp_root:</span><br><span class="line">                nodelst.append(temp_root)</span><br><span class="line">                cur_dist = np.linalg.norm(aimlst - temp_root.point)</span><br><span class="line">                <span class="keyword">if</span> max_dis &gt; cur_dist <span class="keyword">and</span> len(node_k) == k:</span><br><span class="line">                    max_index = node_dist.index(max_dis)</span><br><span class="line">                    node_dist[max_index] = cur_dist</span><br><span class="line">                    node_k[max_index] = temp_root.point</span><br><span class="line">                <span class="keyword">elif</span> len(node_k) &lt; k:</span><br><span class="line">                    node_dist.append(cur_dist)</span><br><span class="line">                    node_k.append(temp_root.point)</span><br><span class="line">    <span class="keyword">return</span> node_k</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单测试下</span></span><br><span class="line">df = np.array([[   <span class="number">7</span>,    <span class="number">1</span>,    <span class="number">2</span>],</span><br><span class="line">               [   <span class="number">3</span>,    <span class="number">4</span>,    <span class="number">1</span>],</span><br><span class="line">               [   <span class="number">3</span>,   <span class="number">54</span>,    <span class="number">6</span>],</span><br><span class="line">            [   <span class="number">8</span>,   <span class="number">97</span>,    <span class="number">4</span>],</span><br><span class="line">                [   <span class="number">3</span>,    <span class="number">5</span>,  <span class="number">345</span>],</span><br><span class="line">               [  <span class="number">23</span>,   <span class="number">16</span>, <span class="number">6534</span>]])</span><br><span class="line"></span><br><span class="line">root = <span class="keyword">None</span></span><br><span class="line">root = createKdTree(root, df)</span><br><span class="line"></span><br><span class="line">KNN(root, [<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>],<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这样就是选出了最近邻,在这先只写到这,代码的思想和实现已经完成,对于用于实际预测的话,可以直接使用一些库中封装好的算法,如果想自己练习的话,只要对上面代码稍作修改即可实现,大家可以自行完成,之后有时间我再来实现.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;K-近邻算法学习&quot;&gt;&lt;a href=&quot;#K-近邻算法学习&quot; class=&quot;headerlink&quot; title=&quot;K-近邻算法学习&quot;&gt;&lt;/a&gt;K-近邻算法学习&lt;/h1&gt;&lt;h2 id=&quot;算法描述&quot;&gt;&lt;a href=&quot;#算法描述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="统计学习方法" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
</feed>
