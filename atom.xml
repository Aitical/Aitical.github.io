<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aitical</title>
  
  <subtitle>未来可期</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.aitical.cn/"/>
  <updated>2019-07-31T11:27:27.000Z</updated>
  <id>http://www.aitical.cn/</id>
  
  <author>
    <name>Aitical</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ShuffleNetV2</title>
    <link href="http://www.aitical.cn/2019/07/31/ShuffleNetV2/"/>
    <id>http://www.aitical.cn/2019/07/31/ShuffleNetV2/</id>
    <published>2019-07-31T11:27:27.000Z</published>
    <updated>2019-07-31T11:27:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<span class="exturl" data-url="aHR0cDovL3h4eC5pdHAuYWMuY24vcGRmLzE4MDcuMTExNjQucGRm" title="http://xxx.itp.ac.cn/pdf/1807.11164.pdf">ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design<i class="fa fa-external-link"></i></span></p><h3 id="模型性能指标"><a href="#模型性能指标" class="headerlink" title="模型性能指标"></a>模型性能指标</h3><p>​ 绝大多数的模型压缩和加速的文章中都使用FLOPs(float-point operations)作为模型的评价指标,用来衡量卷积计算量。但是本文开始就通过一组对比试验指出即使是相同MFOPs的模型，在不同平台上实际的处理速度仍差别很大。<img src="/2019/07/31/ShuffleNetV2/001-ft1.png" alt="GPU和ARM平台上相同MFLOPs的模型处理速度对比"></p>]]></content>
    
    <summary type="html">
    
      本文指出模型加速和压缩不应仅关注计算量(FLOPs)这一个指标，还应关注如MAC(memory access coss)等其他损失。并根据不同方面的损失通过多组实验给予了模型设计时的4点建议。
    
    </summary>
    
      <category term="PaperReading" scheme="http://www.aitical.cn/categories/PaperReading/"/>
    
    
      <category term="CNN" scheme="http://www.aitical.cn/tags/CNN/"/>
    
  </entry>
  
</feed>
