<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aitical的个人小站</title>
  
  <subtitle>一直在走,走灰多少太阳</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-25T16:00:57.510Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>吴刚</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2018/03/25/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2018/03/25/决策树/</id>
    <published>2018-03-25T15:32:44.000Z</published>
    <updated>2018-03-25T16:00:57.510Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-决策树模型"><a href="#1-决策树模型" class="headerlink" title="1. 决策树模型"></a>1. 决策树模型</h2><p>分类决策树模型是一种描述对实例进行分类的属性结构.决策树由结点(node)和有向边(directed edge)组成.结点有两种类型,内部结点(internal node)和叶结点(leaf node).内部结点表示一个特征或属性,也结点表示一个类.<br>用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,将实例分配到其子结点:这是,每一个子结点对应着该特征值的一个取值.如此递归的对实例进行测试并分配,直到达到叶结点.最后将实例分到叶结点的类中.</p><h2 id="2-特征选择-数学基础"><a href="#2-特征选择-数学基础" class="headerlink" title="2. 特征选择(数学基础)"></a>2. 特征选择(数学基础)</h2><p>特征选择在于选取对训练数据具有分类能力的特征.通常特征选择的准则是信息增益(ID3算法)或信息增益比(C4.5算法).</p><h3 id="2-1-熵"><a href="#2-1-熵" class="headerlink" title="2.1 熵"></a>2.1 熵</h3><p>熵(entropy)表示随机变量不确定性的度量.<br>设$X$是一个取有限个值的离散变量,其概率分布为:<br>$$<br>P(X=x_{i})=p_{i}, \space i=1,2,3, \cdots, n<br>$$<br>则随机变量$X$的熵的定义为:<br>$$<br>H(X)=-\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$<br>若$p_{i}=0$则定义$0\space log\space0=0$<br><br>式中的对数以2为底或以$e$为底,这时结果的单位分别称作比特(bit)或者纳特(nat)<br>由定义知,熵只依赖于$X$的分布,而与$X$的取值无关,所以也可以将$X$的熵记作$H(p)$,即:<br>$$<br>H(p) = -\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$<br>熵越大,随机变量的不确定性越大.从定义可验证:<br>$$<br>0\leq H(p)\leq log\space n<br>$$<br>当随机变量只取两个值, 例如$1,0$时,即$X$的分布为:<br>$$<br>P(X=1) = p, \space \space P(X=0) = 1-p, \space 0\leq 1<br>$$<br>熵为:<br>$$<br>H(p) = -p\space log_2\space p-(1-p)log_{2}\space (1-p)<br>$$<br>这时熵$H(p)$随概率$p$的变化曲线如下图所示(单位为比特):</p><p><img src="/2018/03/25/决策树/H.png" alt="H(p)"></p><p>当$p=0$或$p=1$时$H(p)=0$,随机变量完全没有不确定性.当$p=0.5$时,$H(p)=1$,熵取值最大,随机变量不确定性最大.<br>简单的来说,熵就是事件发生概率的<strong>负对数的期望</strong><br>$$<br>E(X) = -\sum_{i=1}^{n}p_{i}\space log\space p_{i}<br>$$</p><h3 id="2-2-条件熵"><a href="#2-2-条件熵" class="headerlink" title="2.2 条件熵"></a>2.2 条件熵</h3><p>设有随机变量$(X,Y)$, 其联合概率分布为:<br>$$<br>P(X=x_{i}, Y=y_{j})=p_{ij}, \space i=1,2,\cdots\,n;\space j=1,2,\cdots,m<br>$$<br>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性,随机变量$X$给定的条件下$Y$的条件熵(conditional entropy)$H(Y|X)$,定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望<br>$$<br>H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})<br>$$<br>这里,$p_{i}=P(X=x_{i}), i=1,2,\cdots,n$<br>当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵与条件熵分别称为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy).此时,如果有0概率,令$0\space log\space0=0$</p><h3 id="2-3-信息增益"><a href="#2-3-信息增益" class="headerlink" title="2.3 信息增益"></a>2.3 信息增益</h3><p>信息增益(information gain)表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度.<br>特征$A$对训练数据集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下经验条件熵$H(D|A)$之差,即<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br>一般地,熵$H(Y)$与条件熵$H(Y|X)$的差称为互信息(mutual information).决策树学习中的信息增益等价于训练数据集中类与特征的互信息.</p><p>决策树学习应用信息增益准则选择特征.给定训练数据集$D$和特征$A$,经验熵$H(D)$表示对数据集$D$进行分类的不确定性,而经验条件熵$H(D|A)$表示特征$A$给定条件下$D$的不确定性,那么他们的差,即信息增益就是表示由于特征$A$而使$D$的分类不确定性减少的程度.<br>根据信息增益准则的特征选择方法是:</p><p>对训练数据集$D$,计算其每个特征的信息增益,并比较他们的大小,选择信息增益最大的特征.</p><p>设训练数据集为$D$,$|D|$表示其样本容量,即样本的个数.</p><p>设有$K$个类$C_{k}, k=1,2,\cdots,K$,$|C_{k}|$是属于类$|C_{k}|$的样本个数, $\sum_{k=1}^{K}|C_{k}|=|D|$.</p><p>设特征$A$有$n$个不同取值$\lbrace a_{1},a_{2},\cdots,a_{n}\rbrace $,根据特征$A$的取值将$D$划分为$n$个子集$D_{1}, D_{2}, \cdots, D_{n}$,$|D_{i}|$为$D_{i}$样本个数,$\sum_{i=1}^{n}|D_{i}|=|D|$.</p><p>记子集$D_{i}$中属于类$C_{k}$的样本集合为$D_{ik}$,即$D_{ik}=D_{i}\cap C_{k}$</p><p>信息增益算法:</p><ol><li>计算数据集$D$的经验熵$DH(D)$<br>$$<br>H(D) = -\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\space\frac{|C_{k}|}{|D|}<br>$$</li><li>计算特征$A$对数据集$D$的经验条件熵<br>$$<br>H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{i=1}^{n}\frac{|D_{ik}|}{|D_{i}|}log_{2}\space\frac{|D_{ik}|}{|D_{i}|}<br>$$</li><li>计算信息增益<br>$$<br>g(D,A) = H(D)-H(D|A)<br>$$</li></ol><p>举个例子:</p><table><thead><tr><th>ID</th><th>age</th><th>hasWork</th><th>hasOwnHouse</th><th>credit</th><th>clazz</th></tr></thead><tbody><tr><td>1</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>2</td><td>青年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>3</td><td>青年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>4</td><td>青年</td><td>是</td><td>是</td><td>一般</td><td>是</td></tr><tr><td>5</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>6</td><td>中年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>7</td><td>中年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>8</td><td>中年</td><td>是</td><td>是</td><td>好</td><td>是</td></tr><tr><td>9</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>10</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>11</td><td>老年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>12</td><td>老年</td><td>否</td><td>是</td><td>好</td><td>是</td></tr><tr><td>13</td><td>老年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>14</td><td>老年</td><td>是</td><td>否</td><td>非常好</td><td>是</td></tr><tr><td>15</td><td>老年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr></tbody></table><p>根据信息增益准则选择最优特征:<br>首先看到数据最终有两类,也就是公式中$k=2$<br>先计算经验熵$H(D)$<br>$$<br>H(D) = -\frac{9}{15}log_{2}\frac{9}{15}-\frac{6}{15}log_{2}\frac{6}{15}=0.971<br>$$<br>然后计算$D$对各个特征的经验条件熵,四个特征分别记作$A_{1}, A_{2}, A_{3}, A_{4}$</p><p>可以看到$A_{1}$特征有3个,恰好是均分的,每个对应5条记录</p><p>$$<br>\begin{align}<br>g(D, A_{1}) &amp;= H(D)-\left[ \frac{5}{15}H(D_{1}) +\frac{5}{15}H(D_{2}) +\frac{5}{15}H(D_{3})\right] \\ &amp;=0.971-\left[ \frac{5}{15}\left( -\frac{2}{15}log_{2}\frac{2}{15}-\frac{3}{15}log_{2}\frac{3}{15}\right)+ \\<br>\frac{5}{15}\left( -\frac{3}{15}log_{2}\frac{3}{15}-\frac{2}{15}log_{2}\frac{2}{15}\right)+ \\<br>\frac{5}{15}\left( -\frac{4}{15}log_{2}\frac{4}{15}-\frac{1}{15}log_{2}\frac{1}{15}\right)\right] \\<br>&amp;=0.971-0.888=0.083<br>\end{align}<br>$$<br>同样的求得<br>$$<br>g(D, A_{2}) = 0.324, \space g(D, A_{3}) = 0.420,\space g(D,A_{4}) =0.363<br>$$<br>比较就可以看出$A_{3}$的信息增益最大,所以选择$A_{3}$作为最优特征</p><h3 id="2-4-信息增益比"><a href="#2-4-信息增益比" class="headerlink" title="2.4 信息增益比"></a>2.4 信息增益比</h3><p>以信息增益作为划分训练数据集的特征,存在偏向于选择取值较多的特征的问题.使用信息增益比(information gain ratio)可以对这一问题进行校正.</p><p>这个地方要多体会一下,具体的看,上述例子中,如果我把ID字段也作为一个特征$A_{0}$进行处理的话,毫无疑问,最优特征一定是$A_{0}$,<br>因为ID字段每一条记录都唯一,也就是说此时的条件经验熵:<br>$$<br>H(D|A_{0}) = -\sum_{i=1}^{15}log_{2}(1)=0<br>$$<br>信息增益取得最大值$H(D)$<br>这也就是说信息增益总是会倾向于取值较多的特征项,尤其是连续性特征,每一个数值都会记作唯一的,这个时候信息增益比就可以进行校正.</p><p>特征$A$对训练数据集$D$的<strong>信息增益比</strong>$g_{R}(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_{A}(D)$之比.即<br>$$<br>g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}<br>$$<br>其中<br>$$<br>H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}<br>$$<br>$n$是特征A的取值个数,$D_{i}$是特征$A$取第$i$个时的样本个数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-决策树模型&quot;&gt;&lt;a href=&quot;#1-决策树模型&quot; class=&quot;headerlink&quot; title=&quot;1. 决策树模型&quot;&gt;&lt;/a&gt;1. 决策树模型&lt;/h2&gt;&lt;p&gt;分类决策树模型是一种描述对实例进行分类的属性结构.决策树由结点(node)和有向边(direct
      
    
    </summary>
    
      <category term="统计学习方法" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>KNN与Kd-tree实现</title>
    <link href="http://yoursite.com/2018/03/21/KNN%E4%B8%8EKd-tree%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/03/21/KNN与Kd-tree实现/</id>
    <published>2018-03-21T14:09:41.000Z</published>
    <updated>2018-03-25T15:32:16.871Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-近邻算法学习"><a href="#K-近邻算法学习" class="headerlink" title="K-近邻算法学习"></a>K-近邻算法学习</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述:"></a>算法描述:</h2><p>KNN原理也就是常说的物以类聚,人以群分.当无法判断当前点所属分类时,可以通过统计它所属的位置特征,衡量它周围的邻居权重,把分配到权重大的一类.</p><p>首先要知道变量之间的距离的表示:<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{p}\right)^{\frac{1}{p}}<br>$$<br>其中p=2时就是欧氏距离<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{2}\right)^{\frac{1}{2}}<br>$$</p><p>p=1时称曼哈顿距离:<br>$$<br>L_{p}(x_{(i)},x_{(j)})=\left( \sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|\right)<br>$$</p><p>p=$\propto$时, 它是各个坐标距离的最大值:<br>$$<br>L_{\propto}(x_{i}, x_{j})=max(|x_{i}^{(l)}-x_{j}^{l}|)<br>$$</p><p>KNN算法中,所选择对象的邻居对象都是已经正确分类的.该方法在定类的决策中只依据最近的一个邻居或者几个来决定所属类别.所以这里面K的取值还是很关键的.太小则容易受到训练数据的噪声影响从而产生过拟合,太大则有可能会误会测试,因为这样会使近邻中包含了远离的点.</p><p>算法描述:</p><ol><li>K(近邻数), D(训练样本集合)</li><li>z(x, y)待测样本</li><li>遍历所有样本数据,计算z到样例之间的距离d</li><li>选择最近的k个样例集合 </li></ol><h2 id="简单的算法使用"><a href="#简单的算法使用" class="headerlink" title="简单的算法使用:"></a>简单的算法使用:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'iris.data'</span>, header=<span class="keyword">None</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]].values</span><br><span class="line">y = df[<span class="number">4</span>].values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = sns.pairplot(df, hue= <span class="number">4</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="keyword">True</span>),plot_kws=dict(s=<span class="number">10</span>),)</span><br></pre></td></tr></table></figure><p><img src="/2018/03/21/KNN与Kd-tree实现/iris.png" alt="iris"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(X_train, y_train, X_test, k)</span>:</span></span><br><span class="line">    y_pre = []</span><br><span class="line">    <span class="keyword">for</span> i, test <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">        neighbour = []</span><br><span class="line">        <span class="keyword">for</span> j, train <span class="keyword">in</span> enumerate(X_train):</span><br><span class="line">            dist = np.linalg.norm(test - train)</span><br><span class="line">            label = y_train[j]</span><br><span class="line">            neighbour.append([dist, label])</span><br><span class="line">        neighbour = np.array(neighbour)</span><br><span class="line">        k_neighbour = neighbour[neighbour[:,<span class="number">0</span>].argsort()][:k][:,<span class="number">1</span>]</span><br><span class="line">        y_pre.append(pd.value_counts(k_neighbour).index[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> y_pre</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_pre, y_test)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(y_pre == y_test)/ len(y_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pre = knn(X_train, y_train, X_test, <span class="number">5</span>)</span><br><span class="line">accuracy_score(y_pre, y_test)</span><br></pre></td></tr></table></figure><h2 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h2><p>上面的例子中采用的是循环遍历的方法实现了距离的计算,在数据量很大的时候这样的遍历显然会带来性能的瓶颈,当然也可以对数据进行向量化计算来提升速度,不过这里我们引入kd树这一种数据结构来存储数据并进行knn</p><p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树是二叉树,表示k维空间的一个划分(partition).构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分,构成一系列的k维超矩形区域,kd树的每个节点对应于一个k维超矩形区域.</p><p><strong>算法: 构造平衡kd树<br></strong><br>    输入: k维空间数据集$T=\lbrace{ x_{1}, x_{2}, \dots, x_{N} \rbrace}$<br> 其中$x_{i} = \left( x_{i}^{(1)}, x_{i}^{(2)}, \dots, x_{i}^{(k)}\right), i=1, 2, \dots, N$<br><br>    输出: kd树</p><p>开始: 构造根节点, 选择$x^{(1)}$为坐标轴,以T中所有实例的$x^{(1)}$坐标的中位数为切分点,将根节点对应的超矩形区域切分成两个子区域.切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现</p><p>由根节点生成深度为1的左,右子结点: 左子结点对应的坐标$x^{(1)}$小于切分点的子区域,右子结点对应的坐标大于切分点的子区域</p><p>将落在切分面上的实例点保存在根节点</p><p>重复: 对于深度为$j$的结点, 选择$x^{(l)}$为切分的坐标轴<br>其中$l=(jmodK)+1$<br>以该结点的区域中所有实例的$x^{(i)}$坐标的中位数为切分点,将该结点对应的超矩形区域切分成两个子区域,切分由通过切分点并垂直于坐标轴$x^{(l)}$的超平面实现<br><br>由根节点生成深度为j+1的左,右子结点: 左子结点对应的坐标$x^{(l)}$小于切分点的子区域,右子结点对应的坐标大于切分点的子区域<br><br>将落在切分面上的实例点保存在根节点</p><p>直到两个子区域没有实例在时停止.</p><p>例:二维空间数据集:<br>$$<br>T=\lbrace{(2, 3)^{T},(5, 4)^{T},(9, 6)^{T},(4, 7)^{T},(8, 1)^{T},(7, 2)^{T}\rbrace}<br>$$</p><p>选择$x^{(1)}$轴,中位数是7,平面$x^{(1)}=7$把空间分成左右两个子矩形</p><p>左矩形中与$x^{(2)}=4$分成两个矩形,右矩形中$x^{(2)}=6$划分</p><p>重复上述操作就得到下面的示意图</p><p><img src="/2018/03/21/KNN与Kd-tree实现/kd-tree1.png" alt="划分"><br>树形结构如下:<br><br><img src="/2018/03/21/KNN与Kd-tree实现/kd-tree2.png" alt="kd-tree"></p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    结点类</span></span><br><span class="line"><span class="string">    ------------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    point: 当前节点的序列</span></span><br><span class="line"><span class="string">    split: 当前结点划分的维度</span></span><br><span class="line"><span class="string">    LL: 当前节点的左孩子</span></span><br><span class="line"><span class="string">    RR: 当前节点的右孩子</span></span><br><span class="line"><span class="string">    ------------</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, point=None, split=None, LL=None, RR=None)</span>:</span></span><br><span class="line">        self.point = point</span><br><span class="line">        self.split = split</span><br><span class="line">        self.left = LL</span><br><span class="line">        self.right = RR</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKdTree</span><span class="params">(root, arraydata)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    生成KD树</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    root: 结点指针</span></span><br><span class="line"><span class="string">    arraydata: 结点所属的划分区域</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    root: 结点</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    length = arraydata.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    dimension = arraydata.shape[<span class="number">1</span>]</span><br><span class="line">    var = np.std(arraydata, axis=<span class="number">0</span>)</span><br><span class="line">    split = np.where(var==var.max())[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    sortdata = arraydata[np.argsort(arraydata[:,split])]</span><br><span class="line">    median = sortdata.shape[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">    point = sortdata[median]</span><br><span class="line">    root = Node(point, split)</span><br><span class="line">    root.left = createKdTree(root.left, sortdata[:median,:] )</span><br><span class="line">    root.right = createKdTree(root.right, sortdata[median+<span class="number">1</span>:,:])</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KNN</span><span class="params">(root, aimlst, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    KNN</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    Parameter:</span></span><br><span class="line"><span class="string">    root: kd-tree根节点</span></span><br><span class="line"><span class="string">    aimlst: 待分类的目标序列</span></span><br><span class="line"><span class="string">    k: 选取的近邻数</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    node_K: 选中的k个近邻列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    node_dist = []</span><br><span class="line">    node_k = []</span><br><span class="line">    nodelst = []</span><br><span class="line">    temp_root = root</span><br><span class="line">    <span class="keyword">while</span> temp_root:</span><br><span class="line">        nodelst.append(temp_root)</span><br><span class="line">        distance = np.linalg.norm(aimlst - temp_root.point)</span><br><span class="line">        <span class="keyword">if</span> len(node_k) &lt; k:</span><br><span class="line">            node_dist.append(distance)</span><br><span class="line">            node_k.append(temp_root.point)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            max_dis = np.max(node_dist)</span><br><span class="line">            <span class="keyword">if</span> distance &lt; max_dis:</span><br><span class="line">                max_index = node_dist.index(max_dis)</span><br><span class="line">                node_dist[max_index] = distance</span><br><span class="line">                node_k[max_index] = temp_root.point</span><br><span class="line">        ss = temp_root.split</span><br><span class="line">        <span class="keyword">if</span> aimlst[ss] &lt;= temp_root.point[ss]:</span><br><span class="line">            temp_root = temp_root.left</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp_root = temp_root.right</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> nodelst:</span><br><span class="line">        back_point = nodelst.pop()</span><br><span class="line">        ss = back_point.split</span><br><span class="line">        max_dis = max(node_dist)</span><br><span class="line">        <span class="keyword">if</span> len(node_k) &lt; k <span class="keyword">or</span> abs(aimlst[ss] - back_point.point[ss])&lt; max_dis:</span><br><span class="line">            <span class="keyword">if</span> aimlst[ss] &lt;= back_point.point[ss]:</span><br><span class="line">                temp_root = back_point.right</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp_root = back_point.left</span><br><span class="line">            <span class="keyword">if</span> temp_root:</span><br><span class="line">                nodelst.append(temp_root)</span><br><span class="line">                cur_dist = np.linalg.norm(aimlst - temp_root.point)</span><br><span class="line">                <span class="keyword">if</span> max_dis &gt; cur_dist <span class="keyword">and</span> len(node_k) == k:</span><br><span class="line">                    max_index = node_dist.index(max_dis)</span><br><span class="line">                    node_dist[max_index] = cur_dist</span><br><span class="line">                    node_k[max_index] = temp_root.point</span><br><span class="line">                <span class="keyword">elif</span> len(node_k) &lt; k:</span><br><span class="line">                    node_dist.append(cur_dist)</span><br><span class="line">                    node_k.append(temp_root.point)</span><br><span class="line">    <span class="keyword">return</span> node_k</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单测试下</span></span><br><span class="line">df = np.array([[   <span class="number">7</span>,    <span class="number">1</span>,    <span class="number">2</span>],</span><br><span class="line">               [   <span class="number">3</span>,    <span class="number">4</span>,    <span class="number">1</span>],</span><br><span class="line">               [   <span class="number">3</span>,   <span class="number">54</span>,    <span class="number">6</span>],</span><br><span class="line">            [   <span class="number">8</span>,   <span class="number">97</span>,    <span class="number">4</span>],</span><br><span class="line">                [   <span class="number">3</span>,    <span class="number">5</span>,  <span class="number">345</span>],</span><br><span class="line">               [  <span class="number">23</span>,   <span class="number">16</span>, <span class="number">6534</span>]])</span><br><span class="line"></span><br><span class="line">root = <span class="keyword">None</span></span><br><span class="line">root = createKdTree(root, df)</span><br><span class="line"></span><br><span class="line">KNN(root, [<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>],<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这样就是选出了最近邻,在这先只写到这,代码的思想和实现已经完成,对于用于实际预测的话,可以直接使用一些库中封装好的算法,如果想自己练习的话,只要对上面代码稍作修改即可实现,大家可以自行完成,之后有时间我再来实现.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;K-近邻算法学习&quot;&gt;&lt;a href=&quot;#K-近邻算法学习&quot; class=&quot;headerlink&quot; title=&quot;K-近邻算法学习&quot;&gt;&lt;/a&gt;K-近邻算法学习&lt;/h1&gt;&lt;h2 id=&quot;算法描述&quot;&gt;&lt;a href=&quot;#算法描述&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="统计学习方法" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习 - python" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-python/"/>
    
  </entry>
  
</feed>
